{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "53c840333f5f40ce9a77704adbc3815e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91feb0e0d99b484286fdf16d00577400",
              "IPY_MODEL_590b5611d93046f19dd3c060f512f3dc",
              "IPY_MODEL_fbaa39a071394aa0bcb5382c59860554",
              "IPY_MODEL_aa4eecbf481d4911a9bae0abfa7675cb",
              "IPY_MODEL_e5097e42280249a4b6ef04bd9c8e1d98"
            ],
            "layout": "IPY_MODEL_2b784180596a4668bef1781272b0acc2"
          }
        },
        "91feb0e0d99b484286fdf16d00577400": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f758d0bacbac4510bc8039f2cbbb5a05",
            "placeholder": "​",
            "style": "IPY_MODEL_c3216a00c7064a5ca82fbe03e80ef70b",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "590b5611d93046f19dd3c060f512f3dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_52934eb2eb004ffe9be0b726779edbba",
            "placeholder": "​",
            "style": "IPY_MODEL_a8f60cee4c404b1e891625a5c3b8d92b",
            "value": ""
          }
        },
        "fbaa39a071394aa0bcb5382c59860554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_c8f2c12d22544677a71b023f8ad5359c",
            "style": "IPY_MODEL_46fcbddb1c9743beb351895772ec8458",
            "value": true
          }
        },
        "aa4eecbf481d4911a9bae0abfa7675cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_e73662e527244be1b506fd85349f2a4d",
            "style": "IPY_MODEL_ebaa4bdc551645e6902bed1cd97e3073",
            "tooltip": ""
          }
        },
        "e5097e42280249a4b6ef04bd9c8e1d98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a33e25f1c07c4f3a883cdc1574694543",
            "placeholder": "​",
            "style": "IPY_MODEL_c15e2a90b956497d9ccd5204427a736f",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "2b784180596a4668bef1781272b0acc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "f758d0bacbac4510bc8039f2cbbb5a05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3216a00c7064a5ca82fbe03e80ef70b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52934eb2eb004ffe9be0b726779edbba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8f60cee4c404b1e891625a5c3b8d92b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8f2c12d22544677a71b023f8ad5359c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46fcbddb1c9743beb351895772ec8458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e73662e527244be1b506fd85349f2a4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebaa4bdc551645e6902bed1cd97e3073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "a33e25f1c07c4f3a883cdc1574694543": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c15e2a90b956497d9ccd5204427a736f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine-tuning Mistral 7B Instruct using qLora and Supervise Finetuning on a multi-turn chat dataset**\n",
        "\n",
        "Welcome to this workshop !\n",
        "\n",
        "In this workshop, we are going to learn how to fine-tune open an LLM ( Mistral-7B ) using QLoRA on a multi-turn chat dataset.\n",
        "\n",
        "In our example, we are going to leverage Hugging Face Transformers, TRL, Accelerate, and PEFT.\n",
        "\n",
        "In Detail you will learn how to:\n",
        "\n",
        "\n",
        "1.   Setup Development Environment\n",
        "2.   Load and prepare the dataset\n",
        "3.   Fine-Tune Mistral 7B with QLoRA\n"
      ],
      "metadata": {
        "id": "ZxO4w_FXb69Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VPHbJRlofAUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup Development Environment\n"
      ],
      "metadata": {
        "id": "xE5rxggisQCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before diving into the fine-tuning process, make sure you have the following prerequisites.\n",
        "\n",
        "GPU: This tutorial can run on a free Google Colab notebook with a GPU. Make sure the GPU is accessible with the following command :"
      ],
      "metadata": {
        "id": "moGl1oTtsbrk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OxqFY9Qmpjc",
        "outputId": "6a461bd6-2e6d-403e-89b7-feb4fe3619d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 17 13:32:32 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then install dependencies :"
      ],
      "metadata": {
        "id": "88gUvJQItjF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch #machine learning framework; we just need the datatype\n",
        "!pip install -q git+https://github.com/huggingface/transformers #huggingface transformers for downloading models weights\n",
        "!pip install -q datasets #huggingface datasets to download datasets from the hub and manipulate them\n",
        "!pip install -q peft #Parameter efficient finetuning - for qLora( quantized Low-Rank Adaptation) Finetuning\n",
        "!pip install -q bitsandbytes #For Model weights quantisation\n",
        "!pip install -q trl #Transformer Reinforcement Learning - For Finetuning using Supervised Fine-tuning\n",
        "!pip install -q wandb -U #Used to monitor the model score during training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0oQ06dftpbl",
        "outputId": "0a3c7649-b129-41b4-9080-2bd6d8a1c07f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.7/174.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.9/133.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.0/100.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.6/248.6 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from pprint import pprint\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "from huggingface_hub import notebook_login\n",
        "from peft import LoraConfig, PeftModel\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from trl import SFTTrainer # For supervised finetuning\n",
        "import bitsandbytes as bnb\n",
        "##for dataset formating:\n",
        "from typing import List, Literal, Optional\n",
        "import random"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo75ECee0KOa",
        "outputId": "8c8bbf60-b2f4-4147-979c-7e905d6ad17b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "# Log in to HF Hub\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332,
          "referenced_widgets": [
            "53c840333f5f40ce9a77704adbc3815e",
            "91feb0e0d99b484286fdf16d00577400",
            "590b5611d93046f19dd3c060f512f3dc",
            "fbaa39a071394aa0bcb5382c59860554",
            "aa4eecbf481d4911a9bae0abfa7675cb",
            "e5097e42280249a4b6ef04bd9c8e1d98",
            "2b784180596a4668bef1781272b0acc2",
            "f758d0bacbac4510bc8039f2cbbb5a05",
            "c3216a00c7064a5ca82fbe03e80ef70b",
            "52934eb2eb004ffe9be0b726779edbba",
            "a8f60cee4c404b1e891625a5c3b8d92b",
            "c8f2c12d22544677a71b023f8ad5359c",
            "46fcbddb1c9743beb351895772ec8458",
            "e73662e527244be1b506fd85349f2a4d",
            "ebaa4bdc551645e6902bed1cd97e3073",
            "a33e25f1c07c4f3a883cdc1574694543",
            "c15e2a90b956497d9ccd5204427a736f"
          ]
        },
        "id": "8gBprqVC0MvC",
        "outputId": "d7b10234-ba8a-4dd0-fd2f-2e14bebd14c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53c840333f5f40ce9a77704adbc3815e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "p4NLr3VyPUCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 2.   Load and prepare the dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "qbQJJKuF0RNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this workshop, we will fine-tune Mistral for a multi-turn chat task.\n",
        "\n",
        "At Hugging-Face we recently released [Zephyr](https://huggingface.co/collections/HuggingFaceH4/zephyr-7b-6538c6d6d5ddd1cbb1744a66)\n",
        "\n",
        "We will be using this [dataset](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k) which was used during the initial training data of zephyr. But since we don't want the training to take forever we'll only sample it to a lower number of examples."
      ],
      "metadata": {
        "id": "kFsFGjYk2YxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ultrachat_dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft\")\n",
        "\n",
        "N = 2500  # replace with the number of rows you want\n",
        "ultrachat_dataset = ultrachat_dataset.select(range(N))\n",
        "ultrachat_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55n7mLUz0Z-9",
        "outputId": "9f063eec-2ae1-42e9-aa13-032e4d70ccc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt', 'prompt_id', 'messages'],\n",
              "    num_rows: 2500\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
        "\n",
        "\n",
        "def apply_chat_template(\n",
        "    example, tokenizer, assistant_prefix=\"<|assistant|>\\n\"\n",
        "):\n",
        "\n",
        "    messages = example[\"messages\"]\n",
        "    # We add an empty system message if there is none\n",
        "    if messages[0][\"role\"] != \"system\":\n",
        "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
        "    example[\"text\"] = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=False\n",
        "    )\n",
        "\n",
        "\n",
        "    return example\n",
        "\n"
      ],
      "metadata": {
        "id": "1oDAD83o5Orj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "# The name of the fine-tuned model. Yes the \"I\" is on purpose, it's lower grade Zephyr\n",
        "new_model = \"Zhephir-7B-sft\"\n",
        "\n",
        "wandb_project_name = \"Zhephir-7B-sft\""
      ],
      "metadata": {
        "id": "jSFatJbz-nON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name\n",
        "\n",
        "    )\n",
        "\n",
        "if tokenizer.chat_template is None:\n",
        "    tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE\n",
        "\n",
        "ultrachat_dataset = ultrachat_dataset.map(apply_chat_template, fn_kwargs={\"tokenizer\": tokenizer})\n",
        "ultrachat_dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbVi-nw48-Bf",
        "outputId": "0726c370-1667-4fe5-cfad-406309fbccb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt', 'prompt_id', 'messages', 'text'],\n",
              "    num_rows: 2500\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset=ultrachat_dataset.train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "FYsFbwAdj1iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index in random.sample(range(len(dataset[\"train\"])), 3):\n",
        "  print(f\"#################################  SAMPLE {index} ##################################################\")\n",
        "  print(f\"\\n\\n{dataset['train'][index]['text']} \\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cz90NnALbkvY",
        "outputId": "a97c467f-7bcc-406c-cacb-06e38b9b8f3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#################################  SAMPLE 226 ##################################################\n",
            "\n",
            "\n",
            "<|system|>\n",
            "</s>\n",
            "<|user|>\n",
            "Looking for quick recipe to enjoy as part of your breakfast? Each tasty Open-Face Cheese and Egg Sandwich features a fried egg, KRAFT SINGLES Light Cheese Slice, baby arugula and grape tomatoes on whole grain toast. Why wait? Try it tomorrow.\n",
            "Melt margarine in large nonstick skillet on medium-low heat.\n",
            "Slip cracked eggs, 1 at a time, into skillet, leaving spaces between eggs; cook 5 min. Or until egg whites are set and yolks are cooked to desired doneness.\n",
            "Place 1 toast slice on each of 4 serving plates. Top with Kraft Singles, arugula, tomatoes and eggs.\n",
            "Can you please provide me with the ingredients required to make an Open-Face Cheese and Egg Sandwich?</s>\n",
            "<|assistant|>\n",
            "- 4 slices of whole grain bread\n",
            "- 4 large eggs\n",
            "- 4 KRAFT SINGLES Light Cheese Slices\n",
            "- 2 cups of baby arugula\n",
            "- 1 cup of grape tomatoes\n",
            "- 1 tablespoon of margarine</s>\n",
            "<|user|>\n",
            "Can you give me a list of other quick breakfast recipes that use KRAFT SINGLES cheese slices?</s>\n",
            "<|assistant|>\n",
            "Yes, here are three quick breakfast recipes that use KRAFT SINGLES cheese slices:\n",
            "\n",
            "1. Cheesy Egg and Bacon Breakfast Sandwich: Fry 2 strips of bacon and set aside. In the same pan, cook one cracked egg, seasoned with salt and pepper. Once the egg is cooked, place one KRAFT SINGLES cheese slice on top and let it melt. Toast a whole grain English muffin and spread with mayonnaise. Add the bacon, egg and cheese and serve.\n",
            "2. Cheesy Breakfast Quesadilla: Spread a tablespoon of salsa on a whole grain tortilla. Add one scrambled egg and one KRAFT SINGLES cheese slice torn into small pieces. Fold the tortilla in half and cook on a skillet on medium heat until cheese is melted and tortilla is crispy. Serve with additional salsa, chopped cilantro and diced avocado.\n",
            "3. Cheesy Breakfast Bagel: Toast a whole grain bagel and spread with cream cheese. Add one KRAFT SINGLES cheese slice torn into small pieces and one sliced tomato. Top with cooked bacon and a fried egg.</s>\n",
            "<|user|>\n",
            "These recipes look great! Can you tell me how many calories are in the Open-Face Cheese and Egg Sandwich?</s>\n",
            "<|assistant|>\n",
            "Sure, the Open-Face Cheese and Egg Sandwich recipe with KRAFT SINGLES Light Cheese Slices, baby arugula and grape tomatoes on whole grain toast is approximately 307 calories per serving. However, the calorie count may differ based on the specific ingredients used and cooking method.</s>\n",
            " \n",
            "\n",
            "\n",
            "#################################  SAMPLE 796 ##################################################\n",
            "\n",
            "\n",
            "<|system|>\n",
            "</s>\n",
            "<|user|>\n",
            "Create a series of TikTok videos showcasing different travel pillow dances that incorporate popular travel destinations and music from that region. Each video should include a unique choreography that highlights the features and benefits of the travel pillow while also incorporating elements of local culture. The style of the videos should be energetic, upbeat, and visually eye-catching, with a focus on capturing the attention of a younger audience. The content should be fun, engaging, and shareable, with a clear call to action encouraging viewers to purchase the travel pillow.</s>\n",
            "<|assistant|>\n",
            "Video 1: \"Tropical Paradise\"\n",
            "- Location: Hawaii\n",
            "- Music: \"Hawaiian Roller Coaster Ride\" by Mark Keali'i Ho'omalu\n",
            "- Choreography: Dancers use the travel pillow to mimic the rolling waves of the ocean, swaying side to side and bobbing up and down. They incorporate hula dancing moves and tropical hand gestures for a fun and playful vibe.\n",
            "\n",
            "Video 2: \"Eurotrip\"\n",
            "- Location: Paris, France\n",
            "- Music: \"La Vie En Rose\" by Edith Piaf\n",
            "- Choreography: Dancers use the travel pillow to show the excitement of exploring the city of love, showcasing landmarks such as the Eiffel Tower and the Arc de Triomphe. They incorporate swinging arm movements and twirls to represent the romantic atmosphere of Paris.\n",
            "\n",
            "Video 3: \"African Adventure\"\n",
            "- Location: Safari in Tanzania\n",
            "- Music: \"Waka Waka\" by Shakira\n",
            "- Choreography: Dancers use the travel pillow to show the thrill of going on safari and encountering wild animals, using movements such as crawling, jumping, and mimicking animal sounds. They incorporate traditional African dance moves for an energetic and rhythmic performance.\n",
            "\n",
            "Video 4: \"Beach Bumming\"\n",
            "- Location: Bali, Indonesia\n",
            "- Music: \"Banana Pancakes\" by Jack Johnson\n",
            "- Choreography: Dancers use the travel pillow to show the relaxation and tranquility of a beach vacation. They incorporate movements such as stretching, sunbathing, and surfing to showcase the full range of activities you can do while using the travel pillow.\n",
            "\n",
            "Video 5: \"City Sights\"\n",
            "- Location: New York City, USA\n",
            "- Music: \"Empire State of Mind\" by Jay-Z featuring Alicia Keys\n",
            "- Choreography: Dancers use the travel pillow to showcase the excitement and energy of the Big Apple, showcasing iconic landmarks such as the Statue of Liberty and Times Square. They incorporate fast and snappy movements to represent the fast-paced nature of the city. \n",
            "\n",
            "Each video ends with a clear call to action, encouraging viewers to purchase the travel pillow for their next adventure. The videos can be shared on TikTok, Instagram, and other social media platforms for a wider reach.</s>\n",
            "<|user|>\n",
            "This is great! I love the different travel destinations and music choices you've come up with. Could you add some more information about the benefits of the travel pillow for each video? Maybe some captions or voiceover explaining how it provides comfort and support during long journeys?</s>\n",
            "<|assistant|>\n",
            "Absolutely! Here are some examples of how we can incorporate information about the benefits of the travel pillow into each video:\n",
            "\n",
            "Video 1: \"Tropical Paradise\"\n",
            "- Caption: \"No more stiff necks during long flights! Our travel pillow provides optimal support for your neck and head, making your tropical adventures more comfortable than ever. #TravelPillow #Hawaii #IslandLife\"\n",
            "\n",
            "Video 2: \"Eurotrip\"\n",
            "- Voiceover: \"Exploring new cities is exciting, but it can be exhausting too. Our travel pillow provides the perfect amount of cushioning for your head and neck, so you can take on the city of love without any discomfort. #Paris #EuroTrip #TravelComfort\"\n",
            "\n",
            "Video 3: \"African Adventure\"\n",
            "- Caption: \"Going on a safari can be a bumpy ride, but our travel pillow makes it feel smooth and comfortable. The ergonomic design provides support for your neck and shoulders, making it easier to spot lions and zebras. #Tanzania #SafariBound #TravelEssentials\"\n",
            "\n",
            "Video 4: \"Beach Bumming\"\n",
            "- Voiceover: \"Relaxing on the beach is a great way to unwind, but it's even better with our travel pillow. The plush material provides a soft and cozy place to rest your head, making those beach naps even more refreshing. #Bali #IslandVibes #TravelRelaxation\"\n",
            "\n",
            "Video 5: \"City Sights\"\n",
            "- Caption: \"Exploring the hustle and bustle of NYC is a once-in-a-lifetime experience, but it can leave you feeling tired and sore. Our travel pillow helps you stay comfortable on long subway rides and hectic sightseeing tours. #NewYorkCity #BigApple #TravelSupport\"\n",
            "\n",
            "By including information about the benefits of the travel pillow in each video, we can better showcase why viewers should consider purchasing one for their next adventure. This can help to drive sales and increase brand awareness.</s>\n",
            " \n",
            "\n",
            "\n",
            "#################################  SAMPLE 53 ##################################################\n",
            "\n",
            "\n",
            "<|system|>\n",
            "</s>\n",
            "<|user|>\n",
            "In vivid detail, describe the sensory experience of the scent of a forest after a rainfall. Your paragraph should not only mention the smell, but also touch upon other senses such as sight and sound, and depict the emotions or memories that this natural phenomenon evokes. Be sure to use sensory language, metaphors, similes, and other literary devices to create a rich and engaging reading experience for your audience.</s>\n",
            "<|assistant|>\n",
            "The scent of a forest after a rainfall is a natural perfume that invigorates the senses. As I walk through the forest, I am immediately struck by the clean, crisp smell that permeates the air. The earthy fragrance is mixed with the sweet aroma of fresh leaves and damp bark. Tiny droplets of water cling to everything, causing the forest to glisten in the sunlight that finally breaks through the clouds. The wetness of the forest creates a squishy sound underfoot as I step over rocks and leaves. The silence is broken only by the occasional rustling of leaves stirred by a gentle breeze blowing through the trees. The forest is alive; the colors are brighter, the greens are greener, and the browns are deeper. It's as if every tree is exhaling a sigh of relief, thrilled to have received an adequate amount of water. The scent is so powerful it triggers a nostalgic feeling within me, reminding me of my childhood, when I would play in the forest near my house after a summer downpour. It's a magical elixir that soothes and refreshes the soul.</s>\n",
            "<|user|>\n",
            "Can you add more about the sound of the raindrops hitting the leaves and forest floor? I think it would enhance the sensory experience even more.</s>\n",
            "<|assistant|>\n",
            "Sure, I would love to! As I continue to stroll through the forest, I can hear the pitter-patter of raindrops hitting the leaves and the forest floor. Each drop creates a delicate sound, as if it's landing on an instrument in a symphony, playing a beautiful melody. The sound is soothing, echoing softly throughout the forest, creating a peaceful atmosphere. The leaves rustle and sway with the gentle breeze that accompanies the rain, and the ground becomes a puddle of rainwater, welcoming the new life that the forest will inevitably produce. It's as if the forest has become a symphony of different sounds, each one harmonizing with the other, creating a beautiful and peaceful melody that washes over me, leaving me completely transfixed. The sound of the rain feels like a beautiful serenade, as if the forest is singing a lullaby to me, putting me into a peaceful slumber.</s>\n",
            "<|user|>\n",
            "That was amazing! Can you add more about the feel of the rain on my skin and how it blends with the earthy scent? I want to fully immerse myself in this sensory experience.</s>\n",
            "<|assistant|>\n",
            "Of course! As I walk through the forest, I stretch my arms out to feel the rain on my skin. The droplets are cool and refreshing, running down my face and seeping through my clothes. The sensation is soft and gentle, like little taps from nature's fingertips. As I continue to walk, I begin to feel the coolness of the rain mingle with the earthy scent. The wet bark and leaves create a perfect backdrop for the smell, and it mixes with the dampness on my skin to create a perfect harmony. It's as if the scent is entwined with the water droplets, creating a new entity made entirely of scent and water. The earthy scent and the rain create a sense of renewal and rejuvenation, as if I'm being cleansed of my worries and anxieties. It's a feeling of pure serenity, and as I close my eyes, I let myself be completely immersed in the sensations surrounding me. Everything seems to have come together to create a symphony of senses, each one complementing the other to bring about a perfect experience.</s>\n",
            "<|user|>\n",
            "Wow, your description of the forest after a rainfall was breathtaking! Can you add more about the animals that might be present in the forest during this time, and how they contribute to the overall sensory experience?</s>\n",
            "<|assistant|>\n",
            "Yes, I can definitely integrate more about the animals present in the forest after a rainfall! As I continue to walk through the forest, I see flashes of movement that catch my eye. Chipmunks and squirrels scurry up trees, their wet fur glistening in the sunlight. Birds flit about, enjoying the drops of rain that cling to their feathers. A woodpecker knocks away at a tree trunk, creating a soft drumbeat that joins the melody of the forest. The forest is alive with activity, and the animals add to the overall experience with their own unique contributions. Each sound and movement they make is a layer upon the harmony of the forest after the rainfall, creating a complex but pleasing tapestry of sensory experience. The rustling of leaves as a rabbit bounds through the underbrush, the chirping of birds as they call out to each other, and the soft murmur of creatures hidden away in the forest add to the sensory experience created by the rainfall. Together, they all create a symphony that's serene, soothing, and comforting. The animals seem to be happy and lively just as the trees and plants, enjoying the nourishment the rainfall brings. Their presence reminds me that the forest is not just a place of beauty, but also a home to so many creatures, each living and thriving in perfect harmony with one another.</s>\n",
            " \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Fine-Tune Mistral 7B with QLoRA\n"
      ],
      "metadata": {
        "id": "0df05TRoc71S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",  # Auto selects device to put model on.\n",
        ")\n",
        "model.config.use_cache = False\n",
        "print(base_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "hiQESHX9szP7",
        "outputId": "b6077f8c-9b07-4b35-8251-efef998eb9ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ca6f83165a8e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m base_model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      2\u001b[0m      \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForCausalLM' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "id": "E-WbnZbpVX7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)"
      ],
      "metadata": {
        "id": "fPC7GwWIVpec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_all_linear_names(model):\n",
        "    cls = bitsandbytes.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "\n",
        "    # lm_head is often excluded.\n",
        "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
        "        lora_module_names.remove('lm_head')\n",
        "    return list(lora_module_names)\n",
        "\n",
        "\n",
        "modules = find_all_linear_names(base_model)\n",
        "print(modules)\n"
      ],
      "metadata": {
        "id": "o7BUNxmrqa06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# QLoRA parameters\n",
        "################################################################################\n",
        "\n",
        "# LoRA attention dimension\n",
        "lora_r = 64\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1\n",
        "\n",
        "#Layers of the NN on which we want to add LoRA Adapters ( it's the ones used in Zephyr)\n",
        "targets=[\n",
        "    \"q_proj\",\n",
        "    \"k_proj\",\n",
        "    \"v_proj\",\n",
        "    \"o_proj\",\n",
        "]\n",
        "#anf these are the all the ones that we can applu LoRA on.\n",
        "targets=modules\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 4\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 4\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2.0e-05\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule (constant a bit better than cosine)\n",
        "lr_scheduler_type = \"constant\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = -1\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 25\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25\n",
        "\n",
        "################################################################################\n",
        "# SFT parameters\n",
        "################################################################################\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = None\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ],
      "metadata": {
        "id": "1D9XA_ijc3ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the base model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "\n",
        "base_model.config.use_cache = False\n",
        "base_model.config.pretraining_tp = 1\n"
      ],
      "metadata": {
        "id": "ba8NcTWDnxwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You set these environment variables for the HF `Trainer`.\n",
        "# But you can also define your run beforehand.\n",
        "os.environ[\"WANDB_PROJECT\"] = wandb_project_name\n",
        "\n",
        "# Log model when running HF Trainer reporting to wandb.\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"true\"  # Apparently this is deprecated in version 5 of transformers.\n",
        "\n",
        "# Use wandb to watch the gradients & model parameters.\n",
        "os.environ[\"WANDB_WATCH\"] = \"all\""
      ],
      "metadata": {
        "id": "ynCu4EaLa-8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = wandb.init(\n",
        "    project=wandb_project_name,  # Project name.\n",
        "    name=\"log_dataset\",          # name of the run within this project.\n",
        "    config={                     # Configuration dictionary.\n",
        "        \"split\": \"train\"\n",
        "    },\n",
        "    group=\"dataset\",             # Group runs. This run belongs in \"dataset\".\n",
        "    tags=[\"dataset\"],            # Tags. More dynamic, low-level grouping.\n",
        "    notes=\"Logging subset of Puffin dataset.\"  # Description about the run.\n",
        ")  # Check out the other parameters in the `wandb.init`!"
      ],
      "metadata": {
        "id": "vwbdw309WOh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    target_modules=targets,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=100, # the number of training steps the model will take\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=base_model,\n",
        "    train_dataset=dataset['train'],\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")"
      ],
      "metadata": {
        "id": "jAf6MlmZoOsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)\n",
        "\n"
      ],
      "metadata": {
        "id": "HwQgN5_woXvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run.finish()"
      ],
      "metadata": {
        "id": "WBunDl_ybhT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Empty VRAM\n",
        "import gc\n",
        "del base_model\n",
        "gc.collect()\n",
        "\n",
        "del trainer\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "I84RqKL2oeQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": 0},\n",
        ")\n",
        "merged_model= PeftModel.from_pretrained(base_model, new_model)\n",
        "merged_model= merged_model.merge_and_unload()\n",
        "\n",
        "# Save the merged model\n",
        "merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
        "tokenizer.save_pretrained(\"merged_model\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "qycE65GBofjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push the model and tokenizer to the Hugging Face Model Hub\n",
        "merged_model.push_to_hub(new_model, use_temp_dir=False)\n",
        "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
      ],
      "metadata": {
        "id": "KB_jTrKFok1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YoW_F_02o2g4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}