{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxO4w_FXb69Y"
   },
   "source": [
    "# **Fine-tuning Mistral 7B Instruct using qLora and Supervise Finetuning on a multi-turn chat dataset**\n",
    "\n",
    "Welcome to this workshop !\n",
    "\n",
    "In this workshop, we are going to learn how to fine-tune open an LLM ( Mistral-7B ) using QLoRA on a multi-turn chat dataset.\n",
    "\n",
    "In our example, we are going to leverage Hugging Face Transformers, TRL, Accelerate, and PEFT.\n",
    "\n",
    "In Detail you will learn how to:\n",
    "\n",
    "\n",
    "1.   Setup Development Environment\n",
    "2.   Load and prepare the dataset\n",
    "3.   Fine-Tune Mistral 7B with QLoRA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xE5rxggisQCG"
   },
   "source": [
    "# 1. Setup Development Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moGl1oTtsbrk"
   },
   "source": [
    "Before diving into the fine-tuning process, make sure you have the following prerequisites.\n",
    "\n",
    "GPU: This tutorial can run on a free Google Colab notebook with a GPU. Make sure the GPU is accessible with the following command :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1OxqFY9Qmpjc",
    "outputId": "6a461bd6-2e6d-403e-89b7-feb4fe3619d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 21 16:52:34 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       On  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   23C    P8               8W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88gUvJQItjF9"
   },
   "source": [
    "And then install dependencies :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D0oQ06dftpbl",
    "outputId": "0a3c7649-b129-41b4-9080-2bd6d8a1c07f"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch #machine learning framework; we just need the datatype\n",
    "!pip install -q git+https://github.com/huggingface/transformers #huggingface transformers for downloading models weights\n",
    "!pip install -q datasets #huggingface datasets to download datasets from the hub and manipulate them\n",
    "!pip install -q peft #Parameter efficient finetuning - for qLora( quantized Low-Rank Adaptation) Finetuning\n",
    "!pip install -q bitsandbytes #For Model weights quantisation\n",
    "!pip install -q trl #Transformer Reinforcement Learning - For Finetuning using Supervised Fine-tuning\n",
    "!pip install -q wandb -U #Used to monitor the model score during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qo75ECee0KOa",
    "outputId": "8c8bbf60-b2f4-4147-979c-7e905d6ad17b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda/lib/python3.11/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import LoraConfig, PeftModel,prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from trl import SFTTrainer # For supervised finetuning\n",
    "import bitsandbytes as bnb\n",
    "##for dataset formating:\n",
    "from typing import List, Literal, Optional\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332,
     "referenced_widgets": [
      "53c840333f5f40ce9a77704adbc3815e",
      "91feb0e0d99b484286fdf16d00577400",
      "590b5611d93046f19dd3c060f512f3dc",
      "fbaa39a071394aa0bcb5382c59860554",
      "aa4eecbf481d4911a9bae0abfa7675cb",
      "e5097e42280249a4b6ef04bd9c8e1d98",
      "2b784180596a4668bef1781272b0acc2",
      "f758d0bacbac4510bc8039f2cbbb5a05",
      "c3216a00c7064a5ca82fbe03e80ef70b",
      "52934eb2eb004ffe9be0b726779edbba",
      "a8f60cee4c404b1e891625a5c3b8d92b",
      "c8f2c12d22544677a71b023f8ad5359c",
      "46fcbddb1c9743beb351895772ec8458",
      "e73662e527244be1b506fd85349f2a4d",
      "ebaa4bdc551645e6902bed1cd97e3073",
      "a33e25f1c07c4f3a883cdc1574694543",
      "c15e2a90b956497d9ccd5204427a736f"
     ]
    },
    "id": "8gBprqVC0MvC",
    "outputId": "d7b10234-ba8a-4dd0-fd2f-2e14bebd14c5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdfb9f5a48134fd0b5a291e49112f041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "# Log in to HF Hub\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "p4NLr3VyPUCh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjofthomas\u001b[0m (\u001b[33msuccess-hf\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbQJJKuF0RNv"
   },
   "source": [
    "\n",
    "\n",
    "# 2.   Load and prepare the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFsFGjYk2YxD"
   },
   "source": [
    "For this workshop, we will fine-tune Mistral for a multi-turn chat task.\n",
    "\n",
    "At Hugging-Face we recently released [Zephyr](https://huggingface.co/collections/HuggingFaceH4/zephyr-7b-6538c6d6d5ddd1cbb1744a66)\n",
    "\n",
    "We will be using this [dataset](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k) which was used during the initial training data of zephyr. But since we don't want the training to take forever we'll only sample it to a lower number of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55n7mLUz0Z-9",
    "outputId": "9f063eec-2ae1-42e9-aa13-032e4d70ccc5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'prompt_id', 'messages'],\n",
       "    num_rows: 2500\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ultrachat_dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft\")\n",
    "\n",
    "N = 2500  # replace with the number of rows you want\n",
    "ultrachat_dataset = ultrachat_dataset.select(range(N))\n",
    "ultrachat_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1oDAD83o5Orj"
   },
   "outputs": [],
   "source": [
    "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "\n",
    "\n",
    "def apply_chat_template(\n",
    "    example, tokenizer, assistant_prefix=\"<|assistant|>\\n\"\n",
    "):\n",
    "\n",
    "    messages = example[\"messages\"]\n",
    "    # We add an empty system message if there is none\n",
    "    if messages[0][\"role\"] != \"system\":\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "\n",
    "    return example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jSFatJbz-nON"
   },
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# The name of the fine-tuned model. Yes the \"I\" is on purpose, it's lower grade Zephyr\n",
    "new_model = \"Zhephir-7B-sft\"\n",
    "\n",
    "wandb_project_name = \"mistral-7B-MT-sft-workshop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WbVi-nw48-Bf",
    "outputId": "0726c370-1667-4fe5-cfad-406309fbccb0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'prompt_id', 'messages', 'text'],\n",
       "    num_rows: 2500\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name\n",
    "\n",
    "    )\n",
    "\n",
    "if tokenizer.chat_template is None:\n",
    "    tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "ultrachat_dataset = ultrachat_dataset.map(apply_chat_template, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "ultrachat_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FYsFbwAdj1iW"
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset=ultrachat_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cz90NnALbkvY",
    "outputId": "a97c467f-7bcc-406c-cacb-06e38b9b8f3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################  SAMPLE 212 ##################################################\n",
      "\n",
      "\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "Given the text: Oes the beauty and majesty of the natural world need to be tied to a higher or more spiritual force to be truly understood? Why or why not?\n",
      "Does the beauty and majesty of the natural world need to be tied to a higher or more spiritual force to be truly understood? Why or why not?\n",
      "you will write an 1100-word research paper using proper APA format and in argumentative mode. The abstract should be 100 words and the body (excluding the cover page and reference page) of the paper should be 1000 words.\n",
      "Oes the balance of power maintain international order rather than peace? Oes the availability of photographs significantly change what we can know about the U.S. Civil War? Oes the aurthor william blakes life reflect in the book The marriage of heaven and helland how. Oes the brand of leisure hotel matter when Chinese customers choose accommodation. Oes The British Prime Minister Inhabit A Presidential Premiership? Oes the Christan salvesens sickness absence management policy meet its obligations under the DDA?\n",
      "Can you provide a detailed argumentative analysis on whether or not the beauty and majesty of the natural world needs to be tied to a higher or more spiritual force to be truly understood?</s>\n",
      "<|assistant|>\n",
      "Abstract:\n",
      "\n",
      "The beauty and majesty of the natural world have been a subject of discussion and contemplation for centuries. Some argue that its true understanding requires connecting it to a higher or more spiritual force, while others see it as a standalone source of wonder and awe. This paper argues that while spirituality can enhance one's appreciation of the natural world, it is not necessary for its understanding. The paper provides evidence from scientific and philosophical perspectives, and concludes that the beauty and majesty of the natural world are in themselves enough to be appreciated without any spiritual or religious connections.\n",
      "\n",
      "Introduction:\n",
      "\n",
      "The natural world is a source of endless fascination and wonder for many people. From towering mountains to serene lakes, from exotic wildlife to glowing sunsets, the beauty and majesty of nature never cease to amaze us. However, the question of whether or not its understanding is tied to a higher or more spiritual force has long been debated. Some argue that connecting the natural world to a spiritual force helps in comprehending its essence, while others see spirituality as irrelevant to understanding nature. This paper explores the arguments on both sides of this debate and sheds light on the significance of spirituality and its relationship with the natural world.\n",
      "\n",
      "Body:\n",
      "\n",
      "Ancient philosophers and religious traditions have explored the relationship between spirituality and nature for centuries. In the Chinese philosophy of Taoism, nature is seen as a manifestation of the Tao, or the ultimate reality. The Tao Te Ching states, “Tao produced the One. The One produced the two. The two produced the three. And the three produced the ten thousand things.” This suggests that the natural world is an expression of the ultimate reality, and therefore, its true understanding requires a connection to the spiritual realm.\n",
      "\n",
      "Similarly, the ancient Greeks believed that nature was infused with the divine, and they personified natural elements such as rivers, mountains, and trees as gods and goddesses. Many Native American cultures also view nature as a spiritual force and connect with the spirits of animals, plants, and natural elements.\n",
      "\n",
      "However, modern society has moved away from such spiritual connections and has developed a more secular understanding of nature. Science has played a significant role in this shift, as it has provided objective explanations of natural phenomena and has helped us understand the workings of the natural world. It has also led to a more materialistic worldview, where nature is seen as a collection of physical objects rather than a manifestation of divinity.\n",
      "\n",
      "Science allows us to understand the beauty and majesty of the natural world without needing spiritual connections. When we look at the night sky, we can appreciate the intricate patterns of the stars and the galaxies, without needing to believe in a higher power. Similarly, we can admire the grandeur of a mountain range or the power of a thunderstorm without needing to connect it to a spiritual force.\n",
      "\n",
      "Moreover, some argue that spirituality can actually detract from the appreciation of nature. When we view nature as a manifestation of a higher power, we can become detached from it and see it as something to be tamed or controlled. In the Christian tradition, for example, nature was often seen as something to be conquered or subdued, rather than something to be cherished and respected. This can lead to environmental degradation and exploitation, as we prioritize our own wants and desires over the natural world that sustains us.\n",
      "\n",
      "Conclusion:\n",
      "\n",
      "In conclusion, the beauty and majesty of the natural world can be appreciated without any spiritual or religious connections. While spirituality may enhance our appreciation of nature, it is not necessary for its understanding. Science has allowed us to comprehend and appreciate the natural world on its own terms, and spirituality can sometimes interfere with our connection with nature. Therefore, we should strive to see nature as a valuable and integral part of our world, without needing to connect it to a higher power.</s>\n",
      "<|user|>\n",
      "Can you provide some examples from the scientific perspective that supports the argument that the beauty and majesty of the natural world can be appreciated without any spiritual or religious connections?</s>\n",
      "<|assistant|>\n",
      "Certainly, here are a few examples from the scientific perspective that support the argument that the beauty and majesty of the natural world can be appreciated without any spiritual or religious connections:\n",
      "\n",
      "1. Evolutionary Biology: From an evolutionary biology perspective, the beauty we find in natural landscapes and wildlife can be explained as a result of our evolutionary history. Our ancestors were naturally drawn to environments that provided necessary resources and protective factors, such as fresh water, food, and shelter. Over time, this attraction has evolved into an aesthetic appreciation of the natural world, even in situations where these resources are no longer necessary for survival.\n",
      "\n",
      "2. Neurobiology: The neural basis of beauty has been studied by neurobiologists, who have found that specific regions of the brain, such as the visual cortex and the prefrontal cortex, are involved in the perception of beauty. They have also found that beauty can evoke emotional responses, such as pleasure, awe, and admiration, which are associated with the release of endorphins and other 'feel-good' chemicals in the brain. These emotional responses contribute to our understanding and appreciation of the natural world.\n",
      "\n",
      "3. Astronomy: Astronomy is a good example of a scientific area where the beauty and majesty of the natural world can be appreciated without any spiritual or religious connections. The awe-inspiring beauty of the galaxies, stars, moons, and planets can all be studied and appreciated without the need for any spiritual or religious explanations. In fact, modern astronomy has unraveled some of the mysteries of the universe, providing us with a deeper understanding of natural phenomena, such as black holes, supernovae, and the Big Bang.\n",
      "\n",
      "4. Environmental Science: Environmental science studies the natural world, but from a human-centric perspective. This scientific field focuses on the interdependence between humans and the natural environment, and how humans can better manage and protect the natural world. Environmental science is fully supportive of the idea that the natural world can be appreciated and understood without the need for spiritual or religious connections.\n",
      "\n",
      "Overall, the scientific perspective supports the idea that the beauty and majesty of the natural world can be appreciated without any spiritual or religious connections. In fact, the scientific approach to understanding nature may actually remove the need for supernatural explanations, instead providing us with rational, empirical, and testable explanations for the natural world around us.</s>\n",
      " \n",
      "\n",
      "\n",
      "#################################  SAMPLE 1844 ##################################################\n",
      "\n",
      "\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "Design a logo and packaging that reflects the high-quality, specialty coffee beans used in the new line of products. Choose a color scheme and font that communicates the brand's sophistication and artisanal approach. Ensure that the packaging is both visually appealing and functional, providing information about the coffee's origin, flavor profile, and brewing recommendations. Consider incorporating eco-friendly packaging materials or other sustainable practices to appeal to eco-conscious consumers.</s>\n",
      "<|assistant|>\n",
      "Logo design: \n",
      "\n",
      "The logo for this specialty coffee brand should reflect its high-quality beans and artisanal approach. One idea is to use an abstract image of a coffee plant or bean, possibly in a metallic or embossed finish to convey a sense of luxury. The font should be sleek and modern, but still easy to read. A sans-serif font like Helvetica or Gotham would work well. The brand name could be written in all caps for a bold and confident look. \n",
      "\n",
      "Color scheme: \n",
      "\n",
      "To communicate sophistication and luxury, the color palette should be rich and deep. A combination of navy blue, gold, and white would be a great choice. Navy blue is a classic color that conveys trustworthiness and stability, while gold is associated with quality and elegance. White will help to create contrast and keep the design from feeling too heavy. \n",
      "\n",
      "Packaging design: \n",
      "\n",
      "The packaging for this line of coffee products should be visually appealing and functional. The bags could feature a textured or embossed finish to add an element of luxury. The navy blue background would be accented with gold and white typography and images. The font should be easy to read, and the product name should stand out. \n",
      "\n",
      "To appeal to eco-conscious consumers, the packaging materials could be made from eco-friendly materials like recycled paper or bio-plastic. The bags could be resealable to keep the coffee fresh, and could also feature a valve to release excess CO2. \n",
      "\n",
      "Information about the coffee's origin, flavor profile, and brewing recommendations should be included on the packaging. This information could be presented in an elegant and informative way, such as a small card inside the bag or on the side of the packaging. \n",
      "\n",
      "Overall, the design should reflect the brand's commitment to high-quality, specialty coffee, while also appealing to consumers who care about sustainability and eco-friendly practices.</s>\n",
      "<|user|>\n",
      "Could you possibly add some suggestions for different coffee roast levels that could be offered in the new line of products?</s>\n",
      "<|assistant|>\n",
      "Sure, I'd be happy to! Different coffee roast levels add variety to a product line, providing options that cater to different tastes and preferences. Here are some suggestions for roast levels that could be included in the new line of products:\n",
      "\n",
      "1. Light roast - This roast level is perfect for coffee drinkers who prefer a milder, more delicate flavor profile. Light roast coffee is characterized by its light brown color and high acidity. It is often fruity and floral in flavor, with hints of citrus. 2. Medium roast - Medium roast coffee is the most popular roast level, with a balance between richness and acidity. Medium roast coffee has a darker color than light roast, but not as dark as dark roast. It offers a smooth and well-rounded flavor, with hints of chocolate, nuts and berries. 3. Dark roast - Dark roast coffee is popular for its strong, bold flavor profile. It has a dark brown color and a low acidity level, which gives it a slightly burnt taste. Dark roast coffee is often described as full-bodied, bold, and smoky. 4. Espresso roast - Espresso roast coffee is made by grinding and packing coffee beans tightly into a small amount of water. This roast level is typically darker and more robust than other roast levels. It offers a rich and intense flavor, with a strong taste of coffee bean oil. Including an option for each of these roast levels in the new line of products can cater to a variety of coffee drinkers, regardless of their preferences.</s>\n",
      " \n",
      "\n",
      "\n",
      "#################################  SAMPLE 1589 ##################################################\n",
      "\n",
      "\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "Create a detailed proposal outlining the logistics, marketing strategy, and budget for a fundraiser event to support a local animal shelter. Include ideas for engaging the community, securing sponsorships, and highlights of the shelter's needs and impact in the community. Present the proposal to the shelter's board of directors for approval and collaborate with them on executing the event.</s>\n",
      "<|assistant|>\n",
      "Introduction\n",
      "\n",
      "We propose to organize a fundraiser event to support a local animal shelter. The event will be held on a Saturday and Sunday, from 9:00 am to 4:00 pm, at a nearby park. Our goal is to raise $20,000 to provide the shelter with the resources they need to take care of the animals in their care.\n",
      "\n",
      "Logistics \n",
      "\n",
      "The fundraiser event will comprise of several activities and attractions throughout the day. Here are the logistics we will need to assemble to make the event a success.\n",
      "\n",
      "Activities & Attractions:\n",
      "\n",
      "1. Pet Adoption Booth: To showcase the adoptable pets from the shelter\n",
      "2. Pet Photo Booth: A selfie booth for pet owners to pose with their pets.\n",
      "3. Pet Show: An opportunity for pet owners to show off their pets’ tricks and talents.\n",
      "4. Craft Fair: A fair for handmade pet crafts and accessories\n",
      "5. Face Painting Booth: For people who want to have vibrant face-painted images of their pets.\n",
      "\n",
      "Marketing Strategy\n",
      "\n",
      "To achieve our goal, we will need to get the word out to as many people as possible about the fundraiser event. Here are the activities we will undertake to market the event:\n",
      "\n",
      "Social Media Campaign: We will create a Facebook Event that we will actively promote throughout the planning phase of the event. We will publish daily posts to keep followers updated on which vendors will be attending, prizes, contests or raffles we are hosting. We will encourage followers to share and invite their friends to attend the event.\n",
      "\n",
      "Flyer Distribution: We will distribute flyers throughout the nearby community. We will share flyers with veterinarians, pet shops, groomers, and in other pet ownership gathering points.\n",
      "\n",
      "Community Outreach: We will partner with radio station, TV networks, and newspapers to promote the event. Some of these media outlets may be willing to donate airtime or editorial space to support our animal shelter fundraising event.\n",
      "\n",
      "Budget\n",
      "\n",
      "To conduct this fundraiser effectively, we would require a budget of $10,000. The budget will be utilized as follows:\n",
      "\n",
      "1. Renting Tables: $1,500\n",
      "2. Public Liability Insurance: $2,000\n",
      "3. Marketing expenses: $1,000\n",
      "4. Payment for Promotional Material (Flyers, Posters): $500\n",
      "5. Professional equipment rentals for PA system: $600\n",
      "6. Volunteers – reimbursement for supplies and miscellaneous items: $2,500\n",
      "7. Donation to the animal shelter: $2,000\n",
      "\n",
      "Securing Sponsorship\n",
      "\n",
      "We will approach various big-box stores, pet supply stores, and veterinary clinics in the local community for potential sponsorships. In exchange for sponsorship, we will promote their company on our social media, as well as flyers, posters, and merchandise available in our store, such as custom t-shirts and hats.\n",
      "\n",
      "Conclusion\n",
      "\n",
      "Our fundraiser event will provide awareness of the animal shelter and vetting needs while connecting animal lovers to a worthwhile cause. With support from the board of directors, we hope to make this fundraiser event a yearly event, which will help maintain the shelter, expand their services and find homes for all the needy animals.</s>\n",
      "<|user|>\n",
      "This proposal looks great! Can you add some more details about the craft fair? What kind of pet crafts and accessories will be available? How many vendors will be participating?</s>\n",
      "<|assistant|>\n",
      "Sure, here are more details about the craft fair at the fundraiser event:\n",
      "\n",
      "The craft fair will feature a variety of handmade pet crafts and accessories made by local vendors. These crafts and accessories will include:\n",
      "\n",
      "1. Pet clothing such as sweaters, shirts, and dresses\n",
      "2. Pet toys made with eco-friendly materials\n",
      "3. Customized pet collars and leashes\n",
      "4. Pet-themed jewelry and accessories for pet owners\n",
      "5. Personalized pet tags\n",
      "\n",
      "We anticipate having 10-12 vendors participate in the craft fair, each with their unique collection of crafts and accessories. We will work with the vendors to ensure that their products are priced competitively to appeal to pet owners looking for unique items for their beloved pets. The craft fair will add another level of fun and engagement to the fundraiser event while also providing a platform for local artisans to showcase their talents.</s>\n",
      "<|user|>\n",
      "Can you suggest some ways to encourage people to participate in the pet show? Also, do you have any ideas for interactive activities that could be added to the event to make it more engaging?</s>\n",
      "<|assistant|>\n",
      "Yes, here are some ways to encourage people to participate in the pet show:\n",
      "\n",
      "1. Promote the Pet Show: To encourage participation in the pet show, we will feature it prominently in our marketing and promotional material. We will post about the pet show on our event page, our social media platforms, and flyers. We will also run ads with photos of previous years' winners to garner interest. 2. Offer Prizes for Best Pet Performances: Incentivize participants by offering a prize to the winner. We will feature multiple categories such as \"Best Costume,\" \"Best Trick,\" \"Best Talent,\" \"Most Obedient,\" \"Best Personality,\" and \"Cutest Couple (owner and pet).\" 3. Provide a Stage for the Participants: Standing on a stage creates a sense of importance and grandeur. We will provide a platform where pet owners can showcase and communicate their pet’s unique qualities. Onlookers will need no further encouragement when they watch the joy and pride emanating from owners. To make the event more engaging, we can implement interactive activities such as:\n",
      "\n",
      "1. Puppy Cuddle Booth: Working alongside the pet shelter, we can make use of some animals that are just too young to be put forward for adoption as a companion for attendees. 2. Pet Agility Course: Set up an agility course for pets to exhibit their skills and mastery of obstacles. It will bring out the playful side of the pets and attract animal lovers. 3. Ask A Vet Booth: A licensed veterinarian can be on-site to answer pet owners' queries, offer free advice, and educate pet owners about their health care, nutrition, and preventive medicine. 4. Pet Spa Center: A mobile grooming unit can offer pet owners the chance to pamper their pets with services such as nail trimming, bathing, and other pet grooming services. These interactive activities can be advertised to the public via flyers and social media, which adds an extra element of education, fun, and connection to the event.</s>\n",
      "<|user|>\n",
      "I love the idea of the Pet Spa Center! Can you give me more information about the services that will be provided? Will there be any costs associated with these services, or will they be free? Also, how many pets can be accommodated at once?</s>\n",
      "<|assistant|>\n",
      "Sure thing! The Pet Spa Center will feature a mobile grooming unit that will offer a variety of services for pets, such as nail trimming, bathing, and grooming. The unit will feature a licensed groomer who specializes in handling and catering to animals' grooming requirements. \n",
      "\n",
      "The services offered at the Pet Spa Center will be free, but we will encourage attendees to make a donation to the animal shelter. Attendees who make donations over $20 will receive a free pet spa coupon that they can use during their next visit to the mobile grooming unit.\n",
      "\n",
      "The number of pets accommodated at once will depend on the size and capacity of the mobile grooming unit and may vary depending on the size and temperament of the pet. The groomer will work diligently to ensure as many pets as possible can be accommodated. We will also encourage attendees to sign up for Pet Spa services early to ensure they can be accommodated. \n",
      "\n",
      "We believe that the Pet Spa Center will attract a lot of pet owners who enjoy want their pets to live their best life. Having a grooming center at the event is a great way to engage with attendees while supporting our animal shelter's mission of advocating for the welfare of the animals in our community.</s>\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index in random.sample(range(len(dataset[\"train\"])), 3):\n",
    "  print(f\"#################################  SAMPLE {index} ##################################################\")\n",
    "  print(f\"\\n\\n{dataset['train'][index]['text']} \\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0df05TRoc71S"
   },
   "source": [
    "# 3. Fine-Tune Mistral 7B with QLoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "id": "hiQESHX9szP7",
    "outputId": "b6077f8c-9b07-4b35-8251-efef998eb9ca"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4be0a3b9b6548d895417925c03ba924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "      # Auto selects device to put model on.\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "o7BUNxmrqa06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v_proj', 'k_proj', 'gate_proj', 'down_proj', 'up_proj', 'q_proj', 'o_proj']\n"
     ]
    }
   ],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = torch.nn.Linear #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "\n",
    "    # lm_head is often excluded.\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "print(modules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1D9XA_ijc3ax"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "#Layers of the NN on which we want to add LoRA Adapters ( it's the ones used in Zephyr)\n",
    "targets=[\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "]\n",
    "#anf these are the all the ones that we can applu LoRA on.\n",
    "targets=modules\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2.0e-05\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 25\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map=\"auto\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ba8NcTWDnxwN"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092cf9afbc84456e8c59d43e22a3d1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the base model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ynCu4EaLa-8-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# You set these environment variables for the HF `Trainer`.\n",
    "# But you can also define your run beforehand.\n",
    "os.environ[\"WANDB_PROJECT\"] = wandb_project_name\n",
    "\n",
    "# Log model when running HF Trainer reporting to wandb.\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"true\"  # Apparently this is deprecated in version 5 of transformers.\n",
    "\n",
    "# Use wandb to watch the gradients & model parameters.\n",
    "os.environ[\"WANDB_WATCH\"] = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "vwbdw309WOh8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/wandb/run-20231121_165437-obhgsfki</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/success-hf/mistral-7B-MT-sft-workshop/runs/obhgsfki' target=\"_blank\">train_run0</a></strong> to <a href='https://wandb.ai/success-hf/mistral-7B-MT-sft-workshop' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/success-hf/mistral-7B-MT-sft-workshop' target=\"_blank\">https://wandb.ai/success-hf/mistral-7B-MT-sft-workshop</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/success-hf/mistral-7B-MT-sft-workshop/runs/obhgsfki' target=\"_blank\">https://wandb.ai/success-hf/mistral-7B-MT-sft-workshop/runs/obhgsfki</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project=wandb_project_name,\n",
    "    name=\"train_run0\",  # Sometimes I use the run name as short descriptor for the run.\n",
    "    config={\n",
    "        \"split\": \"train\",\n",
    "        # Optionally, you can add all hyperparameters and configs here for better reproducibility!\n",
    "    },\n",
    "    group=\"train\",\n",
    "    tags=[\"train\", \"AdamW\"],  # Add tags for what might characterize this run.\n",
    "    notes=\"Initial finetuning.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jAf6MlmZoOsE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:194: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2822d9fb53ce403289ac4300c485aeb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    target_modules=targets,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=100, # the number of training steps the model will take\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HwQgN5_woXvH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/ubuntu/miniconda/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/100 00:49 < 1:19:53, 0.02 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBunDl_ybhT3"
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I84RqKL2oeQd"
   },
   "outputs": [],
   "source": [
    "# Empty VRAM\n",
    "import gc\n",
    "del model\n",
    "gc.collect()\n",
    "\n",
    "del trainer\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qycE65GBofjD"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "merged_model= PeftModel.from_pretrained(model, new_model)\n",
    "merged_model= merged_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KB_jTrKFok1G"
   },
   "outputs": [],
   "source": [
    "# Push the model and tokenizer to the Hugging Face Model Hub\n",
    "merged_model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YoW_F_02o2g4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2b784180596a4668bef1781272b0acc2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "46fcbddb1c9743beb351895772ec8458": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "52934eb2eb004ffe9be0b726779edbba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53c840333f5f40ce9a77704adbc3815e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_91feb0e0d99b484286fdf16d00577400",
       "IPY_MODEL_590b5611d93046f19dd3c060f512f3dc",
       "IPY_MODEL_fbaa39a071394aa0bcb5382c59860554",
       "IPY_MODEL_aa4eecbf481d4911a9bae0abfa7675cb",
       "IPY_MODEL_e5097e42280249a4b6ef04bd9c8e1d98"
      ],
      "layout": "IPY_MODEL_2b784180596a4668bef1781272b0acc2"
     }
    },
    "590b5611d93046f19dd3c060f512f3dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_52934eb2eb004ffe9be0b726779edbba",
      "placeholder": "​",
      "style": "IPY_MODEL_a8f60cee4c404b1e891625a5c3b8d92b",
      "value": ""
     }
    },
    "91feb0e0d99b484286fdf16d00577400": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f758d0bacbac4510bc8039f2cbbb5a05",
      "placeholder": "​",
      "style": "IPY_MODEL_c3216a00c7064a5ca82fbe03e80ef70b",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "a33e25f1c07c4f3a883cdc1574694543": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8f60cee4c404b1e891625a5c3b8d92b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aa4eecbf481d4911a9bae0abfa7675cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_e73662e527244be1b506fd85349f2a4d",
      "style": "IPY_MODEL_ebaa4bdc551645e6902bed1cd97e3073",
      "tooltip": ""
     }
    },
    "c15e2a90b956497d9ccd5204427a736f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c3216a00c7064a5ca82fbe03e80ef70b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c8f2c12d22544677a71b023f8ad5359c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5097e42280249a4b6ef04bd9c8e1d98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a33e25f1c07c4f3a883cdc1574694543",
      "placeholder": "​",
      "style": "IPY_MODEL_c15e2a90b956497d9ccd5204427a736f",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "e73662e527244be1b506fd85349f2a4d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebaa4bdc551645e6902bed1cd97e3073": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "f758d0bacbac4510bc8039f2cbbb5a05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fbaa39a071394aa0bcb5382c59860554": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_c8f2c12d22544677a71b023f8ad5359c",
      "style": "IPY_MODEL_46fcbddb1c9743beb351895772ec8458",
      "value": true
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
