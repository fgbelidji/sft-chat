{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c963ddf",
   "metadata": {},
   "source": [
    "# Building a German MT Conversation Dataset\n",
    "\n",
    "Datasets that we examined:\n",
    "\n",
    "#### [OpenAssistant](https://huggingface.co/datasets/OpenAssistant/oasst1)\n",
    "- Human-generated assistant-style conversation corpus crowd-sourced by over 13,500 volunteers.\n",
    "- Over 10,000 conversations trees\n",
    "- 3k messages in German\n",
    "- Must be processed to reform the conversation trees\n",
    "\n",
    "#### [UltraChat-200k / not used](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)\n",
    "- Original [UltraChat](https://github.com/thunlp/UltraChat) is a huge conversational dataset entirely generated by ChatGPT, even the initial questions.\n",
    "- This is a filtered version of UltraChat: truecasing, correction of grammatical errors, removal of unhelpful assistant answers.\n",
    "- Over 500,000 rows\n",
    "- Sadly only English - no examples in German, but can be translated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc987635",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install fasttext-langdetect\n",
    "!pip install transformers[sentencepiece]\n",
    "!pip install torch\n",
    "!pip install more-itertools\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307e6ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from ftlangdetect import detect\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import requests\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e1c0b5",
   "metadata": {},
   "source": [
    "### Load OpenAssistant and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45df60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_assistant = datasets.load_dataset(\"A-Roucher/Open_Assistant_Conversation_Chains\")\n",
    "open_assistant = open_assistant['train']\n",
    "\n",
    "open_assistant_de = open_assistant.filter(lambda l: l['lang'] == 'de')\n",
    "open_assistant_en = open_assistant.filter(lambda l: l['lang'] == 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1886da",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_messages = open_assistant_en['messages']\n",
    "\n",
    "def transcribe_to_list(conversation):\n",
    "    return [message['content'] for message in conversation]\n",
    "    \n",
    "all_messages_list = []\n",
    "for conversation in tqdm(all_messages):\n",
    "    all_messages_list += transcribe_to_list(conversation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498eb5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_code(message):\n",
    "    suspicious = ['):\\n', ';\\n', '//', ' # ', 'def ', '{}', 'const ', 'var ', '.delete', '.add', '/>', '</', '==', '!=', 'if __']\n",
    "    return any([el in message for el in suspicious]) or ('example' in message and ('code' in message or 'script' in message))\n",
    "\n",
    "open_assistant_en = open_assistant_en.map(lambda example: {'could_be_code': any([detect_code(text['content'].lower()) for text in example['messages']])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95b0ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_assistant_en = open_assistant_en.filter(lambda example: not example['could_be_code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92145389",
   "metadata": {},
   "source": [
    "# Translate to German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b166eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = 'https://ecfcd7jkenav3ri3.us-east-1.aws.endpoints.huggingface.cloud'\n",
    "bearer_token = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bf5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {bearer_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67643c7b",
   "metadata": {},
   "source": [
    "### Single thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c4642",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n",
    "MAX_LEN_TOKENIZED = 508\n",
    "\n",
    "\n",
    "def split_long_message(text, max_len):\n",
    "    words = iter(text.split())\n",
    "    lines, current = [], next(words)\n",
    "    for word in words:\n",
    "        if len(current) + 1 + len(word) > max_len:\n",
    "            lines.append(current)\n",
    "            current = word\n",
    "        else:\n",
    "            current += \" \" + word\n",
    "    lines.append(current)\n",
    "    return lines\n",
    "\n",
    "def split_if_too_long(message, tokenizer, max_len_tokenized=MAX_LEN_TOKENIZED, max_len_text=500):\n",
    "    tokenized = tokenizer.encode(message)\n",
    "    if len(tokenized) > max_len_tokenized:\n",
    "        return split_long_message(message, max_len_text)\n",
    "    else:\n",
    "        return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e419b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(payload):\n",
    "    # payload[\"model\"] = {'image': {'custom': {'env': {\"MAX_CONCURRENT_REQUESTS\": MAX_WORKERS, 'CUDA_LAUNCH_BLOCKING':'1'}}}}\n",
    "    response = requests.post(API_URL, headers=HEADERS, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def translate(message):\n",
    "    return query({\n",
    "        \"inputs\": message,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddba6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(['Hello I eat.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47c7d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_conversation([(False, 'Hello I eat.'), (True, ['Hello, I eat.', 'Hello, I eat.'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e049aae8",
   "metadata": {},
   "source": [
    "### With concurrent requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e562ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n",
    "\n",
    "all_conversations = open_assistant_en['messages'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from aiohttp import ClientSession, ClientTimeout\n",
    "import time\n",
    "\n",
    "async def request(document, semaphore):\n",
    "    # Semaphore guard\n",
    "    async with semaphore:\n",
    "        payload = {\n",
    "            \"inputs\": document['content'],\n",
    "            \"truncate\": True,\n",
    "            'CUDA_LAUNCH_BLOCKING':'1',\n",
    "            \"model\": {'image': {'custom': {'env': {\"MAX_CONCURRENT_REQUESTS\": \"512\", 'CUDA_LAUNCH_BLOCKING':'1'}}}}\n",
    "        }\n",
    "        \n",
    "        timeout = ClientTimeout(total=200)  # Set a timeout for requests (10 seconds here)\n",
    "\n",
    "        async with ClientSession(timeout=timeout, headers=HEADERS) as session:\n",
    "            async with session.post(API_URL, json=payload) as resp:\n",
    "                #if resp.status != 200:\n",
    "                #    raise RuntimeError(await resp.text())\n",
    "                try:\n",
    "                    result = await resp.json()\n",
    "                except:\n",
    "                    print(resp.text())\n",
    "        try:\n",
    "            if isinstance(document['content'], list):\n",
    "                document['translation'] = ''.join([el['translation_text'] for el in result])\n",
    "            else:\n",
    "                document['translation'] = result[0]['translation_text']\n",
    "            return result\n",
    "        except:\n",
    "            print(\"Error on\", document)\n",
    "\n",
    "async def call_all(conversations):\n",
    "    # Semaphore to limit concurrent requests. Adjust the number as needed.\n",
    "    semaphore = asyncio.BoundedSemaphore(16)\n",
    "\n",
    "    # Creating a list of tasks\n",
    "    output = []\n",
    "    for convo in conversations:\n",
    "        for document in convo:\n",
    "            document['content'] = split_if_too_long(document['content'], tokenizer)\n",
    "            output.append(request(document, semaphore))\n",
    "    \n",
    "    # Using tqdm to show progress. It's been integrated into the async loop.\n",
    "    for f in tqdm(asyncio.as_completed(output), total=len(output)):\n",
    "        await f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "\n",
    "await call_all(all_conversations)\n",
    "\n",
    "# Print elapsed time\n",
    "elapsed_time = time.perf_counter() - start\n",
    "minutes, seconds = divmod(elapsed_time, 60)\n",
    "print(f\"{int(minutes)} min {seconds:.2f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24ebe6f",
   "metadata": {},
   "source": [
    "### Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fca79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pickle\n",
    "\n",
    "cop = copy.deepcopy(all_conversations)\n",
    "\n",
    "with open('data.obj', 'wb') as file:\n",
    "    pickle.dump(cop, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9dea97",
   "metadata": {},
   "source": [
    "Check missing translations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b476ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "for el in cop:\n",
    "    for submessage in el:\n",
    "        if 'translation' not in submessage.keys() or len(submessage['translation']) == 0:\n",
    "            counter+=1\n",
    "print(\"Number of missing translations:\", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_messages = []\n",
    "for el in cop:\n",
    "    conv = []\n",
    "    for submessage in el:\n",
    "        conv.append({'role':submessage['role'], 'content':submessage['translation']})\n",
    "    translated_messages.append(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b64edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_assistant_en = open_assistant_en.add_column('messages_german', translated_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d608aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_assistant_en = open_assistant_en.rename_column(\"messages\", \"messages_original\")\n",
    "open_assistant_en = open_assistant_en.rename_column(\"lang\", \"lang_original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ee0e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6593845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_assistant_en_2.push_to_hub('A-Roucher/Open_Assistant_Chains_German_Translation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31558315",
   "metadata": {},
   "source": [
    "# Test Inference Endpoint performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7ab42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "n_messages=1\n",
    "t = time.time()\n",
    "for i in range(n_messages):\n",
    "    print(translate(all_messages_list[i]))\n",
    "print((time.time() - t)/n_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62e1e3",
   "metadata": {},
   "source": [
    "Batched version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1816eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "small = Dataset.from_dict(open_assistant_en[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd668a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_assistant_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b99a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in tqdm(all_messages_list):\n",
    "    tokenizer.encode(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce24821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_lengths = [len(message) for message in all_messages_list]\n",
    "all_lengths = pd.Series(all_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caeae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lengths.min(), all_lengths.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138805ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in all_messages_list:\n",
    "    if len(message) < 10:\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839fcbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_assistant_en = open_assistant_en.map(lambda example: {'translated_messages': translate_conversation(example['messages'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f30ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_assistant_en.save_to_disk(\"translations.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5edf4d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from more_itertools import chunked\n",
    "\n",
    "t=time.time()\n",
    "\n",
    "with open('translation2.txt', 'w') as output_file:\n",
    "    for batch in tqdm(chunked(all_messages_list, 8)):\n",
    "        for el in translate(batch):\n",
    "            output_file.write(f\"{el['translation_text']}\\n\")\n",
    "\n",
    "print((time.time() - t)/n_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35c3ac2",
   "metadata": {},
   "source": [
    "Scales more or less linearly, so chaining messages does not help. A this pace it would take 10hours to translate everything: so we have to switch to a faster model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a0f7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open_assistant_en = open_assistant_en.map(lambda example: {'messages_de': translate_conversation(example['messages'])})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42947b4e",
   "metadata": {},
   "source": [
    "### Test local translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e187692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"translation\",\n",
    "    model= 'Helsinki-NLP/opus-mt-en-de',\n",
    ")\n",
    "n_examples = 12\n",
    "\n",
    "# KeyDataset(dataset, \"text\")\n",
    "t = time.time()\n",
    "for out in tqdm(pipe(all_messages_list[:n_examples], batch_size=4)):\n",
    "    print(out)\n",
    "print((time.time()-t)/n_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7894bf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt', 'w') as f:\n",
    "    for line in lines:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a802b3b0",
   "metadata": {},
   "source": [
    "### NLLB Distilled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44055b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\", src_lang=\"en\", tgt_lang='de')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbe21f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentences):\n",
    "    outputTranslation = []\n",
    "    # Sentence-by-sentence\n",
    "    # for sentence in sentences:\n",
    "    #     inputs = tokenizer([sentence], return_tensors=\"pt\", padding=True)\n",
    "    #     output_sequences = model.generate(\n",
    "    #         input_ids=inputs[\"input_ids\"],\n",
    "    #         attention_mask=inputs[\"attention_mask\"],\n",
    "    #         do_sample=False,  # disable sampling to test if batching affects output\n",
    "    #         forced_bos_token_id=tokenizer.lang_code_to_id['deu_Latn']\n",
    "    #         )\n",
    "    #     outputTranslation += tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n",
    "\n",
    "    # Group of sentences\n",
    "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "    output_sequences = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            do_sample=False,  # disable sampling to test if batching affects output\n",
    "            forced_bos_token_id=tokenizer.lang_code_to_id['deu_Latn']\n",
    "        )\n",
    "    outputTranslation += tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n",
    "    \n",
    "    print(\"### \" + \" Output Translation\")\n",
    "    for sentence in outputTranslation:\n",
    "        print(sentence)\n",
    "\n",
    "    print(\"#### translate() returning.\")\n",
    "    return outputTranslation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5e974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=time.time()\n",
    "n_examples = 12\n",
    "translate(all_messages_list[:n_examples])\n",
    "print((time.time()-t)/n_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177d5f92",
   "metadata": {},
   "source": [
    "10.23s/example without batching, 4.66 with it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10ccfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"translation\",\n",
    "    model= 'facebook/nllb-200-distilled-600M', #'Helsinki-NLP/opus-mt-en-de',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
