{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c963ddf",
   "metadata": {},
   "source": [
    "# Building a German MT Conversation Dataset\n",
    "\n",
    "Datasets that we examined:\n",
    "\n",
    "#### [UltraChat-200k / not used](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)\n",
    "- Original [UltraChat](https://github.com/thunlp/UltraChat) is a huge conversational dataset entirely generated by ChatGPT, even the initial questions.\n",
    "- This is a filtered version of UltraChat: truecasing, correction of grammatical errors, removal of unhelpful assistant answers.\n",
    "- Over 500,000 rows\n",
    "- Sadly only English - no examples in German, but can be translated.\n",
    "\n",
    "\n",
    "#### [OpenAssistant / selected](https://huggingface.co/datasets/OpenAssistant/oasst1)\n",
    "- Human-generated assistant-style conversation corpus crowd-sourced by over 13,500 volunteers.\n",
    "- Over 10,000 conversations trees\n",
    "- 3k messages in German\n",
    "- Must be processed to reform the conversation trees.\n",
    "- As this is entierly human-generated, we think examples are better quality than in UltraChat - and since it is large enough we will use only this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc987635",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install datasets\n",
    "!pip install transformers[sentencepiece]\n",
    "!pip install more-itertools\n",
    "!pip install matplotlib\n",
    "!pip install huggingface-hub\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307e6ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import copy\n",
    "import requests\n",
    "import getpass\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from aiohttp import ClientSession, ClientTimeout\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e1c0b5",
   "metadata": {},
   "source": [
    "### Load OpenAssistant and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45df60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_assistant = datasets.load_dataset(\"A-Roucher/Open_Assistant_Conversation_Chains\")\n",
    "open_assistant = open_assistant['train']\n",
    "\n",
    "open_assistant_de = open_assistant.filter(lambda l: l['lang'] == 'de')\n",
    "open_assistant_en = open_assistant.filter(lambda l: l['lang'] == 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1886da",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_messages = open_assistant_en['messages']\n",
    "\n",
    "def transcribe_to_list(conversation):\n",
    "    return [message['content'] for message in conversation]\n",
    "    \n",
    "all_messages_list = []\n",
    "for conversation in tqdm(all_messages):\n",
    "    all_messages_list += transcribe_to_list(conversation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498eb5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_code(message):\n",
    "    suspicious = ['):\\n', ';\\n', '//', ' # ', 'def ', '{}', 'const ', 'var ', '.delete', '.add', '/>', '</', '==', '!=', 'if __']\n",
    "    return any([el in message for el in suspicious]) or ('example' in message and ('code' in message or 'script' in message))\n",
    "\n",
    "open_assistant_en = open_assistant_en.map(lambda example: {'could_be_code': any([detect_code(text['content'].lower()) for text in example['messages']])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95b0ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_assistant_en = open_assistant_en.filter(lambda example: not example['could_be_code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92145389",
   "metadata": {},
   "source": [
    "# Translate to German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b166eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = 'https://ecfcd7jkenav3ri3.us-east-1.aws.endpoints.huggingface.cloud'\n",
    "bearer_token = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bf5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {bearer_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e419b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=HEADERS, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def translate(message):\n",
    "    return query({\n",
    "        \"inputs\": message,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67643c7b",
   "metadata": {},
   "source": [
    "### Single thread test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddba6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(['Hello there.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e049aae8",
   "metadata": {},
   "source": [
    "### With concurrent requests\n",
    "Here we take care to limit each request under the max number of tokens accepted by the model. Else it crashed the inference endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e562ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n",
    "\n",
    "all_conversations = open_assistant_en['messages'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c4642",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN_TOKENIZED = 508\n",
    "\n",
    "def split_long_message(text, max_len):\n",
    "    words = iter(text.split())\n",
    "    lines, current = [], next(words)\n",
    "    for word in words:\n",
    "        if len(current) + 1 + len(word) > max_len:\n",
    "            lines.append(current)\n",
    "            current = word\n",
    "        else:\n",
    "            current += \" \" + word\n",
    "    lines.append(current)\n",
    "    return lines\n",
    "\n",
    "def split_if_too_long(message, tokenizer, max_len_tokenized=MAX_LEN_TOKENIZED, max_len_text=500):\n",
    "    tokenized = tokenizer.encode(message)\n",
    "    if len(tokenized) > max_len_tokenized:\n",
    "        return split_long_message(message, max_len_text)\n",
    "    else:\n",
    "        return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def request(document, semaphore):\n",
    "    # Semaphore guard\n",
    "    async with semaphore:\n",
    "        payload = {\n",
    "            \"inputs\": document['content'],\n",
    "            \"truncate\": True,\n",
    "            'CUDA_LAUNCH_BLOCKING':'1',\n",
    "            \"model\": {'image': {'custom': {'env': {\"MAX_CONCURRENT_REQUESTS\": \"512\", 'CUDA_LAUNCH_BLOCKING':'1'}}}}\n",
    "        }\n",
    "        \n",
    "        timeout = ClientTimeout(total=200)  # Set a timeout for requests (10 seconds here)\n",
    "\n",
    "        async with ClientSession(timeout=timeout, headers=HEADERS) as session:\n",
    "            async with session.post(API_URL, json=payload) as resp:\n",
    "                #if resp.status != 200:\n",
    "                #    raise RuntimeError(await resp.text())\n",
    "                try:\n",
    "                    result = await resp.json()\n",
    "                except:\n",
    "                    print(resp.text())\n",
    "        try:\n",
    "            if isinstance(document['content'], list):\n",
    "                document['translation'] = ''.join([el['translation_text'] for el in result])\n",
    "            else:\n",
    "                document['translation'] = result[0]['translation_text']\n",
    "            return result\n",
    "        except:\n",
    "            print(\"Error on\", document)\n",
    "\n",
    "async def call_all(conversations):\n",
    "    # Semaphore to limit concurrent requests. Adjust the number as needed.\n",
    "    semaphore = asyncio.BoundedSemaphore(16)\n",
    "\n",
    "    # Creating a list of tasks\n",
    "    output = []\n",
    "    for convo in conversations:\n",
    "        for document in convo:\n",
    "            document['content'] = split_if_too_long(document['content'], tokenizer)\n",
    "            output.append(request(document, semaphore))\n",
    "    \n",
    "    # Using tqdm to show progress. It's been integrated into the async loop.\n",
    "    for f in tqdm(asyncio.as_completed(output), total=len(output)):\n",
    "        await f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "\n",
    "await call_all(all_conversations)\n",
    "\n",
    "# Print elapsed time\n",
    "elapsed_time = time.perf_counter() - start\n",
    "minutes, seconds = divmod(elapsed_time, 60)\n",
    "print(f\"{int(minutes)} min {seconds:.2f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24ebe6f",
   "metadata": {},
   "source": [
    "### Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fca79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_conversations = copy.deepcopy(all_conversations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9dea97",
   "metadata": {},
   "source": [
    "Check missing translations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b476ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "for el in copy_conversations:\n",
    "    for submessage in el:\n",
    "        if 'translation' not in submessage.keys() or len(submessage['translation']) == 0:\n",
    "            counter+=1\n",
    "print(\"Number of missing translations:\", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_messages = []\n",
    "for el in copy_conversations:\n",
    "    conv = []\n",
    "    for submessage in el:\n",
    "        conv.append({'role':submessage['role'], 'content':submessage['translation']})\n",
    "    translated_messages.append(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b64edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_assistant_en = open_assistant_en.add_column('messages_german', translated_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ee0e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6593845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_assistant_en.push_to_hub('A-Roucher/Open_Assistant_Chains_German_Translation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
